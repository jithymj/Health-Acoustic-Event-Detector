# -*- coding: utf-8 -*-
"""hear_1006.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10EgBCgN1vz-DmKzHvIBvOUqo-Oj_KiUN
"""

!pip install timm==0.4.5

from google.colab import drive
drive.mount('/content/drive')

import os
import torch
import torchaudio
import numpy as np
import matplotlib.pyplot as plt
from IPython.display import Audio, display

# ========== Configuration ==========
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
NUM_CLASSES = 3
CLASS_NAMES = ['cough', 'sneeze', 'no_event']  # Update if your classes are different
MODEL_PATH = '/content/drive/MyDrive/HeAR/best_ssast_model.pth'  # Update with your actual path
PRETRAINED_PATH = '/content/drive/MyDrive/ssast-pretrain/SSAST-Base-Patch-400.pth'  # Update with your actual path
SAMPLE_RATE = 16000
SEGMENT_LENGTH = 2  # seconds

# ========== 1. Import Your AST Model ==========
# First, make sure your ASTModel implementation is accessible
import sys
# Add directory containing your model.py to Python path
sys.path.append('/content/drive/MyDrive/models')  # UPDATE THIS PATH

from ast_models import ASTModel  #

# ========== 2. Load Model Without DataParallel Fix ==========
def load_ssast_model(model_path, num_classes=NUM_CLASSES):
    # Initialize model with same configuration as training
    model = ASTModel(
        label_dim=num_classes,
        fshape=16,
        tshape=16,
        fstride=10,
        tstride=10,
        input_fdim=128,
        input_tdim=1024,
        model_size='base',
        pretrain_stage=False,
        load_pretrained_mdl_path=PRETRAINED_PATH  # We're loading fine-tuned weights
    )

    # Load state dictionary directly (no DataParallel prefix)
    state_dict = torch.load(model_path, map_location=DEVICE)
    model.load_state_dict(state_dict)

    model = model.to(DEVICE)
    model.eval()  # Set to evaluation mode

    return model

# Load your trained model
model = load_ssast_model(MODEL_PATH)

# ========== 3. Audio Processing Class (Matches Training Preprocessing) ==========
class AudioProcessor:
    def __init__(self, sample_rate=16000, n_mels=128, target_length=1024):
        self.sample_rate = sample_rate
        self.n_mels = n_mels
        self.target_length = target_length
        self.mel_transform = torchaudio.transforms.MelSpectrogram(
            sample_rate=sample_rate,
            n_fft=1024,
            win_length=1024,
            hop_length=160,
            n_mels=n_mels
        )

    def process_waveform(self, waveform):
        """Process a waveform tensor to spectrogram format"""
        # Generate Mel spectrogram
        mel = self.mel_transform(waveform)

        # Log transform with clipping
        log_mel = torch.log(torch.clamp(mel, min=1e-10))

        # Standardize per-spectrogram
        mean = log_mel.mean()
        std = log_mel.std()
        normalized = (log_mel - mean) / (std + 1e-9)

        # Adjust time dimension
        current_length = normalized.shape[2]
        if current_length < self.target_length:
            pad = self.target_length - current_length
            normalized = torch.nn.functional.pad(normalized, (0, pad))
        else:
            normalized = normalized[:, :, :self.target_length]

        return normalized.unsqueeze(0)  # Add batch dimension

    def load_and_prepare_audio(self, audio_path):
        """Load audio, resample, convert to mono, and split into segments"""
        # Load audio
        waveform, orig_sr = torchaudio.load(audio_path)

        # Resample to 16kHz
        if orig_sr != self.sample_rate:
            resampler = torchaudio.transforms.Resample(orig_sr, self.sample_rate)
            waveform = resampler(waveform)

        # Convert to mono
        if waveform.dim() > 1 and waveform.shape[0] > 1:
            waveform = torch.mean(waveform, dim=0, keepdim=True)

        # Calculate segment parameters
        total_samples = waveform.shape[1]
        segment_samples = int(SEGMENT_LENGTH * self.sample_rate)
        num_segments = int(np.ceil(total_samples / segment_samples))

        # Split into segments
        segments = []
        segment_times = []

        for i in range(num_segments):
            start = i * segment_samples
            end = min((i + 1) * segment_samples, total_samples)

            # Extract segment
            segment = waveform[:, start:end]

            # Pad if shorter than segment length
            if segment.shape[1] < segment_samples:
                pad = segment_samples - segment.shape[1]
                segment = torch.nn.functional.pad(segment, (0, pad))

            # Store segment and its time range
            segments.append(segment)
            segment_times.append((start/self.sample_rate, end/self.sample_rate))

        return segments, segment_times, waveform, self.sample_rate

# Initialize processor
processor = AudioProcessor()

# ========== 4. Prediction Function ==========
def predict_audio_logits(audio_path):
    """Process audio and return logits for each segment"""
    # Load and split audio
    segments, segment_times, full_waveform, sample_rate = processor.load_and_prepare_audio(audio_path)

    # Store results
    results = []

    # Process each segment
    for segment, (start_time, end_time) in zip(segments, segment_times):
        # Convert to spectrogram
        input_tensor = processor.process_waveform(segment).to(DEVICE)

        # Run inference
        with torch.no_grad():
            output = model(input_tensor, task="ft_avgtok")

        # Get logits
        logits = output.squeeze().cpu().numpy()
        predicted_class_idx = torch.argmax(output).item()
        predicted_class = CLASS_NAMES[predicted_class_idx]

        # Store results
        results.append({
            'start_time': start_time,
            'end_time': end_time,
            'logits': logits,
            'predicted_class': predicted_class,
            'raw_output': output.cpu().numpy()
        })

    return results, full_waveform, sample_rate

# ========== 5. Visualization Function ==========
def visualize_results(audio_path, results, full_waveform, sample_rate):
    """Visualize audio and logits over time"""
    # Display audio player
    print(f"\nAnalyzing audio: {os.path.basename(audio_path)}")
    print(f"Total duration: {full_waveform.shape[1]/sample_rate:.2f} seconds")
    display(Audio(full_waveform.numpy().squeeze(), rate=sample_rate))

    # Create figure
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), sharex=True)

    # Plot waveform
    time_axis = np.linspace(0, full_waveform.shape[1]/sample_rate, full_waveform.shape[1])
    ax1.plot(time_axis, full_waveform.numpy().squeeze(), alpha=0.7)
    ax1.set_title("Audio Waveform", fontsize=14)
    ax1.set_ylabel("Amplitude", fontsize=12)
    ax1.grid(alpha=0.3)

    # Add segment markers
    for result in results:
        ax1.axvline(x=result['start_time'], color='r', linestyle='--', alpha=0.5)
        ax1.axvline(x=result['end_time'], color='r', linestyle='--', alpha=0.5)

    # Plot logits over time
    time_points = [(r['start_time'] + r['end_time'])/2 for r in results]

    for i, class_name in enumerate(CLASS_NAMES):
        logit_values = [r['logits'][i] for r in results]
        ax2.plot(time_points, logit_values, marker='o', label=class_name)

    ax2.set_title("Logits Over Time", fontsize=14)
    ax2.set_ylabel("Logit Value", fontsize=12)
    ax2.set_xlabel("Time (seconds)", fontsize=12)
    ax2.legend()
    ax2.grid(alpha=0.3)

    # Add predicted class annotations
    for result in results:
        mid_time = (result['start_time'] + result['end_time'])/2
        ax2.text(mid_time, max(result['logits'])+0.5, result['predicted_class'],
                 ha='center', fontsize=9, bbox=dict(facecolor='white', alpha=0.7))

    plt.tight_layout()
    plt.show()

    # Print detailed results
    print("\nSegment Results:")
    print(f"{'Segment':^10} | {'Start':^8} | {'End':^8} | {'Prediction':^15} | {'Logits':^30}")
    print("-"*80)
    for i, result in enumerate(results):
        logit_str = ", ".join([f"{cls}: {logit:.2f}" for cls, logit in zip(CLASS_NAMES, result['logits'])])
        print(f"{i+1:^10} | {result['start_time']:>6.2f}s | {result['end_time']:>6.2f}s | {result['predicted_class']:^15} | {logit_str}")

    return results

# ========== 6. Example Usage ==========
if __name__ == "__main__":
    # Path to your test audio
    audio_path = "/content/drive/MyDrive/HeAR/EvaluationDataset/236.wav"  # Update with your audio path

    # Process audio and get logits for each segment
    results, full_waveform, sample_rate = predict_audio_logits(audio_path)

    # Visualize results
    visualize_results(audio_path, results, full_waveform, sample_rate)

    # Export results to CSV
    import csv
    csv_path = os.path.splitext(audio_path)[0] + "_logits.csv"
    with open(csv_path, 'w', newline='') as csvfile:
        fieldnames = ['segment', 'start_time', 'end_time', 'predicted_class'] + CLASS_NAMES
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

        writer.writeheader()
        for i, result in enumerate(results):
            row = {
                'segment': i+1,
                'start_time': result['start_time'],
                'end_time': result['end_time'],
                'predicted_class': result['predicted_class']
            }
            # Add logits for each class
            for j, cls in enumerate(CLASS_NAMES):
                row[cls] = result['logits'][j]
            writer.writerow(row)

    print(f"\nResults saved to: {csv_path}")