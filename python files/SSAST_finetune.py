# -*- coding: utf-8 -*-
"""SSAST_0706.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TBWQBqfM7ieDlLObo99-iTM3_ooc2iaj
"""

!pip install timm==0.4.5

# ========== 1. Mount Google Drive ==========
from google.colab import drive
drive.mount('/content/drive')

# ========== 2. Set Paths ==========
import sys
import os

# Paths in your Google Drive
MODELS_PATH = "/content/drive/MyDrive/models"
METADATA_PATH = "/content/drive/MyDrive/ssast-metadata"
PRETRAINED_MODEL_PATH = "/content/drive/MyDrive/ssast-pretrain/SSAST-Base-Patch-400.pth"
SPECTROGRAM_ZIP = "/content/drive/MyDrive/spectrograms.zip"
SPECTROGRAM_PATH = "/content"  # Temporary Colab folder
SPECTROGRAM_PATH1 = "/content/spectrograms"  # Temporary Colab folder

# Add models folder to path and import ASTModel
sys.path.append(MODELS_PATH)
from ast_models import ASTModel

# ========== 3. Unzip Spectrograms ==========
import zipfile

with zipfile.ZipFile(SPECTROGRAM_ZIP, 'r') as zip_ref:
    zip_ref.extractall(SPECTROGRAM_PATH)

print("Spectrograms unzipped to:", SPECTROGRAM_PATH)

# ========== 4. Imports ==========
import numpy as np
import pandas as pd
from tqdm import tqdm
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns
from torch.cuda.amp import autocast, GradScaler

# ========== 5. Config ==========
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
NUM_CLASSES = 3
BATCH_SIZE = 16
EPOCHS = 50
LR = 1e-4

# ========== 6. Dataset Class ==========
class SpectrogramDataset(Dataset):
    def __init__(self, df, root_dir):
        self.df = df
        self.root_dir = root_dir

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        label = int(row['label'])
        file_path = os.path.join(self.root_dir, row['label_name'], row['filename'] + ".npy")
        spec = np.load(file_path).astype(np.float32)
        spec = torch.tensor(spec)

        #  Normalize the spectrogram: (spec - mean) / std
        mean = spec.mean()
        std = spec.std()
        spec = (spec - mean) / (std + 1e-6)

        spec = spec.unsqueeze(0)  # [1, 128, 1024]
        return spec, label

# ========== 7. Load Metadata ==========
def load_metadata(split):
    df = pd.read_csv(os.path.join(METADATA_PATH, f"{split}.csv"))
    label_df = pd.read_csv(os.path.join(METADATA_PATH, "labels.csv"))
    label_map = dict(zip(label_df['class'], label_df['index']))
    df['label'] = df['label'].map(label_map)
    df['label_name'] = df['label'].map({v: k for k, v in label_map.items()})
    return df

train_df = load_metadata("train")
val_df = load_metadata("val")
test_df = load_metadata("test")

# ========== 8. Create Dataloaders ==========
train_loader = DataLoader(SpectrogramDataset(train_df, SPECTROGRAM_PATH1), batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(SpectrogramDataset(val_df, SPECTROGRAM_PATH1), batch_size=BATCH_SIZE)
test_loader = DataLoader(SpectrogramDataset(test_df, SPECTROGRAM_PATH1), batch_size=BATCH_SIZE)

# ========== 9. Initialize Model ==========
model = ASTModel(label_dim=NUM_CLASSES,
                 fshape=16, tshape=16, fstride=10, tstride=10,
                 input_fdim=128, input_tdim=1024,
                 model_size='base',
                 pretrain_stage=False,
                 load_pretrained_mdl_path=PRETRAINED_MODEL_PATH).to(DEVICE)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=LR)
best_val_acc = 0.0
best_model_path = "best_ssast_model.pth"

# ========== 10. Evaluation Function ==========
def evaluate(loader):
    model.eval()
    y_true, y_pred = [], []
    with torch.no_grad():
        for x, y in loader:
            x, y = x.to(DEVICE), y.to(DEVICE)
            outputs = model(x, task="ft_avgtok")
            y_true.extend(y.cpu().numpy())
            y_pred.extend(outputs.argmax(1).cpu().numpy())
    return y_true, y_pred

from torch.optim.lr_scheduler import ReduceLROnPlateau
scaler = GradScaler()
scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)

patience = 5
no_improve_epochs = 0

for epoch in range(EPOCHS):
    model.train()
    running_loss = 0.0

    for x, y in tqdm(train_loader, desc=f"Epoch {epoch+1}/{EPOCHS}"):
        x, y = x.to(DEVICE), y.to(DEVICE)
        optimizer.zero_grad()

        with autocast():
            outputs = model(x, task="ft_avgtok")
            loss = criterion(outputs, y)

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()

        running_loss += loss.item()

        # Clean up memory
        del x, y, outputs, loss
        torch.cuda.empty_cache()

    # ===== Validation =====
    val_y, val_pred = evaluate(val_loader)
    val_acc = accuracy_score(val_y, val_pred)
    print(f"Epoch {epoch+1}: Val Acc = {val_acc:.4f}, Loss = {running_loss:.4f}")
    print(classification_report(val_y, val_pred))

    # Let scheduler step after validation
    scheduler.step(val_acc)

    # ===== Early Stopping Check =====
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        torch.save(model.state_dict(), best_model_path)
        print(" New best model saved.")
        no_improve_epochs = 0
    else:
        no_improve_epochs += 1
        print(f" No improvement for {no_improve_epochs} epoch(s).")

    if no_improve_epochs >= patience:
        print(f" Early stopping triggered after {patience} epochs with no improvement.")
        break

# ========== 12. Final Test Evaluation ==========
model.load_state_dict(torch.load(best_model_path))
test_y, test_pred = evaluate(test_loader)
print("Test Results:")
print(classification_report(test_y, test_pred))

# Confusion Matrix
cm = confusion_matrix(test_y, test_pred)
plt.figure(figsize=(5, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

import zipfile
from google.colab import files  # âœ… Fix: import files module

# Zip the model file
with zipfile.ZipFile('best_ssast_model.zip', 'w') as zipf:
    zipf.write('best_ssast_model.pth')

# Download the zip file
files.download('best_ssast_model.zip')

from google.colab import files
files.download("best_ssast_model.pth")